# Aprendizaje por Refuerzo ‚Äì SARSA, Q-Learning y Softmax
### Diplomatura en Ciencia de Datos ‚Äì FAMAF 2025

---

## üß† Descripci√≥n del proyecto

Este repositorio contiene el Trabajo Pr√°ctico de Aprendizaje por Refuerzo, donde se implementaron y analizaron los algoritmos:

- SARSA (on-policy)
- Q-Learning (off-policy)
- Pol√≠tica de exploraci√≥n Softmax
- Ajuste de hiperpar√°metros Œ±, Œµ y Œ≥
- Visualizaci√≥n de pol√≠ticas aprendidas en el entorno CliffWalking-v1

Tambi√©n se exploraron los entornos CartPole y MountainCar para observar c√≥mo interact√∫a un agente con el entorno en episodios aleatorios.

### ‚úî CartPole-v0  
Un carrito que debe balancear un palo vertical durante el mayor tiempo posible.

<img width="588" height="390" alt="image" src="https://github.com/user-attachments/assets/7ba277e9-0682-4b37-84a9-b11899e993ee" />

### ‚úîÔ∏è MountainCar-v0

Un auto atrapado en un valle que debe tomar impulso para subir la monta√±a.

<img width="585" height="379" alt="image" src="https://github.com/user-attachments/assets/287a1d89-3cf6-4080-92bf-c60df8933af4" />

---
# üß© Implementaciones realizadas

## ‚úî Pol√≠tica Œµ-greedy

El agente elige la mejor acci√≥n conocida con probabilidad (1‚àíŒµ), y una acci√≥n aleatoria con probabilidad Œµ.

```python
def choose_action_e_greedy(state, actions, q, hyperparameters, random_state):
    ...
```
## ‚úî Pol√≠tica Softmax

Se implement√≥ la pol√≠tica Softmax:

‚Äã

```python
def choose_action_softmax(state, actions, q, hyperparameters, random_state):
    ...
```

## üîß Algoritmos implementados
- **SARSA** con pol√≠tica *Œµ-greedy*  
- **Q-Learning** con pol√≠tica *Œµ-greedy*  
- **Softmax** implementado sobre **SARSA** (porque es un algoritmo *on-policy* y la pol√≠tica afecta directamente el aprendizaje)

---
##  üë©‚Äçüíª Autora
Micaela Zamorano

Diplomatura en Ciencia de Datos ‚Äì FAMAF

2025
